{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Importing Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "from sklearn import preprocessing\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import svm\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from tensorflow.keras.models import Sequential,Model\r\n",
    "from tensorflow.keras.layers import Dense,Flatten,InputLayer,Conv2D,MaxPooling2D,Dropout,MaxPool2D,Softmax,ReLU,GlobalAveragePooling2D\r\n",
    "from tensorflow.keras.activations import softmax\r\n",
    "from tensorflow.keras.applications.resnet import ResNet50\r\n",
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalization and adding noises"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mat2gray(img):\r\n",
    "    A = np.double(img)\r\n",
    "    out = np.zeros(A.shape, np.double)\r\n",
    "    normalized = cv2.normalize(A, out, 1.0, 0.0, cv2.NORM_MINMAX)\r\n",
    "    return out\r\n",
    "def random_noise(image, mode='gaussian', seed=None, clip=True, **kwargs):\r\n",
    "    image = mat2gray(image)\r\n",
    "    mode = mode.lower()\r\n",
    "    if image.min() < 0:\r\n",
    "        low_clip = -1\r\n",
    "    else:\r\n",
    "        low_clip = 0\r\n",
    "    if seed is not None:\r\n",
    "        np.random.seed(seed=seed)\r\n",
    "    if mode == 'gaussian':\r\n",
    "        noise = np.random.normal(kwargs['mean'], kwargs['var'] ** 0.5,image.shape)        \r\n",
    "        out = image  + noise\r\n",
    "    if clip:        \r\n",
    "        out = np.clip(out, low_clip, 1.0)\r\n",
    "    return out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing of dataset with gabor filter and augumentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ksize =18  #Use size that makes sense to the image and fetaure size. Large may not be good. \r\n",
    "           #On the synthetic image it is clear how ksize affects imgae (try 5 and 50)\r\n",
    "sigma = 1.5 #Large sigma on small features will fully miss the features. \r\n",
    "theta = 3*np.pi/4 #/4 shows horizontal 3/4 shows other horizontal. Try other contributions\r\n",
    "lamda = 5  #1/4 works best for angled. \r\n",
    "gamma=1.5 #Value of 1 defines spherical. Calue close to 0 has high aspect ratio\r\n",
    "        #Value of 1, spherical may not be ideal as it picks up features from other regions.\r\n",
    "phi = 0 #Phase offset. I leave it to 0. (For hidden pic use 0.8)\r\n",
    "kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, phi, ktype=cv2.CV_32F)\r\n",
    "kernel_resized = cv2.resize(kernel, (400, 400))  # Resize image\r\n",
    "\r\n",
    "img_data_array=[]\r\n",
    "class_name=[]\r\n",
    "#give path only till train folder\r\n",
    "img_folder=\"C:\\\\Users\\\\luhar\\\\emotion\\\\emotion classifier\\\\ck+\"\r\n",
    "for dir1 in os.listdir(img_folder):\r\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\r\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\r\n",
    "            image= cv2.imread( image_path)\r\n",
    "            image=cv2.resize(image,(126,126))\r\n",
    "            \r\n",
    "            ##------------------------------Apply Gabour filter here-----------------------------##\r\n",
    "            image = cv2.filter2D(image, cv2.CV_8UC3, kernel)\r\n",
    "            \r\n",
    "            ##------------------------------Code for Gabour filter Complete here_------------------##\r\n",
    "            image=np.array(image)\r\n",
    "            image = image.astype('float32')\r\n",
    "            image /= 255 \r\n",
    "            img_data_array.append(image)\r\n",
    "            img1=imagenet_utils\r\n",
    "            #img1 = random_noise(image,'gaussian', mean=0.1,var=0.1)\r\n",
    "            img_data_array.append(img1)\r\n",
    "            class_name.append(dir1)\r\n",
    "            class_name.append(dir1)\r\n",
    "label_encoder = preprocessing.LabelEncoder()\r\n",
    "class_name= label_encoder.fit_transform(class_name)\r\n",
    "img_data_array=np.array(img_data_array)\r\n",
    "img_data_array[0].shape\r\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data_array,class_name,test_size=0.1,shuffle=True)\r\n",
    "print(x_train.shape)\r\n",
    "print(x_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing of dataset without gabor filter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_data_array=[]\r\n",
    "class_name=[]\r\n",
    "#give path only till train folder\r\n",
    "img_folder=\"C:\\\\Users\\\\luhar\\\\OneDrive\\\\Documents\\\\Code with ShiviSandy\\\\emotion_classifier\\\\ck+\"\r\n",
    "for dir1 in os.listdir(img_folder):\r\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\r\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\r\n",
    "            image= cv2.imread( image_path)\r\n",
    "            image=cv2.resize(image,(126,126))\r\n",
    "            image=np.array(image)\r\n",
    "            image = image.astype('float32')\r\n",
    "            image /= 255 \r\n",
    "            img_data_array.append(image)\r\n",
    "            class_name.append(dir1)\r\n",
    "label_encoder = preprocessing.LabelEncoder()\r\n",
    "class_name= label_encoder.fit_transform(class_name)\r\n",
    "img_data_array=np.array(img_data_array)\r\n",
    "img_data_array[0].shape\r\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data_array,class_name,test_size=0.1,shuffle=True)\r\n",
    "print(x_train.shape)\r\n",
    "print(x_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Training dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(img_data_array[3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add the different models here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Model No 1\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=Sequential([\r\n",
    "        Conv2D(32,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\",input_shape=img_data_array[0].shape),\r\n",
    "        Conv2D(64,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\"),\r\n",
    "        MaxPooling2D(pool_size=(2,2)),\r\n",
    "        Conv2D(128,kernel_size=(3,3),activation=\"relu\",padding=\"SAME\"),\r\n",
    "        Dropout(0.5),\r\n",
    "        MaxPooling2D(pool_size=(2,2)),\r\n",
    "        Flatten(),\r\n",
    "        Dense(1024,activation=\"relu\"),\r\n",
    "        Dropout(0.5),\r\n",
    "        Dense(512,activation=\"relu\"),\r\n",
    "        Dropout(0.5),\r\n",
    "        Dense(7,activation=\"softmax\")\r\n",
    "    ])\r\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\n",
    "history=model.fit(x_train,y_train,validation_split=0.1,epochs=30,shuffle=True)\r\n",
    "frame = pd.DataFrame(history.history)\r\n",
    "print(frame)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Custom Model no 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=Sequential([\r\n",
    "    Conv2D(6,kernel_size=1,input_shape=img_data_array[0].shape),\r\n",
    "    ReLU(),\r\n",
    "    MaxPooling2D((2,2)),\r\n",
    "    Conv2D(16,kernel_size=1),\r\n",
    "    ReLU(),\r\n",
    "    MaxPooling2D((2,2)),\r\n",
    "    Conv2D(120,kernel_size=5),\r\n",
    "    ReLU(),\r\n",
    "    Dropout(0.3),\r\n",
    "    Flatten(),\r\n",
    "    Dense(84,activation='relu'),\r\n",
    "    Dropout(0.3),\r\n",
    "    Dense(7,activation='softmax')\r\n",
    "])\r\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\n",
    "history=model.fit(x_train,y_train,validation_split=0.1,epochs=30)\r\n",
    "frame = pd.DataFrame(history.history)\r\n",
    "print(frame)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=ResNet50(input_shape=img_data_array[0].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frame=frame.iloc[0:27]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frame=frame.iloc[1:21]\r\n",
    "frame"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Fitting\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frame1=frame.plot(y=\"loss\",title=\"Epocs vs loss\")\r\n",
    "frame1.set(xlabel=\"Epocs\",ylabel=\"loss\")\r\n",
    "frame1.set()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frame.plot( y=[\"accuracy\",\"val_accuracy\"], figsize=(9, 8))\r\n",
    "plt.title(\"Epoch vs accuracy\")\r\n",
    "plt.xlabel(\"# of Epoch\")\r\n",
    "plt.ylabel(\"Accuracy\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss,acc=model.evaluate(x_test,y_test,verbose=2)\r\n",
    "print(loss)\r\n",
    "print(acc)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "d465d72bb9e7834e44b633b17b36e1f56be332276ee2509d196ad2ec1e0a733a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}