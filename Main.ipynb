{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importing important Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CV2' :- Module to do Computer Vision Task\n",
    "import cv2\n",
    "# 'NUMPY' :- MOdule to do operation on array\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# 'PANDAS':- Modules to do operation on Dataframes\n",
    "import pandas as pd\n",
    "# 'SKlearn' :- To do operation on Datasets and taking other Algorithm for training\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'MATPLOTLIB':- Module to draw graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing functions from tensorflow for building and training the model\n",
    "# It is used to do operation on NN models. \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,InputLayer,Conv2D,MaxPooling2D,Dropout,MaxPool2D,Softmax,ReLU,GlobalAveragePooling2D\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION OF IMAGES\n",
    "def mat2gray(img):\n",
    "    A = np.double(img)\n",
    "    out = np.zeros(A.shape, np.double)\n",
    "    # MIN_MAX Normalization\n",
    "    normalized = cv2.normalize(A, out, 1.0, 0.0, cv2.NORM_MINMAX)\n",
    "    return out\n",
    "# ADDED GAUSSIAN RANDOM NOISE FOR AGUMENTATION\n",
    "def random_noise(image, mode='gaussian', seed=None, clip=True, **kwargs):\n",
    "    image = mat2gray(image)\n",
    "    mode = mode.lower()\n",
    "    if image.min() < 0:\n",
    "        low_clip = -1\n",
    "    else:\n",
    "        low_clip = 0\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "    if mode == 'gaussian':\n",
    "        noise = np.random.normal(kwargs['mean'], kwargs['var'] ** 0.5,image.shape)        \n",
    "        out = image  + noise\n",
    "    if clip:        \n",
    "        out = np.clip(out, low_clip, 1.0)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of dataset with gabor filter and augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Gabor Filter\n",
    "\n",
    "ksize =18  \n",
    "sigma = 1.5  \n",
    "theta = 3*np.pi/4 \n",
    "lamda = 5  \n",
    "gamma=1.5 \n",
    "phi = 0\n",
    "kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, phi, ktype=cv2.CV_32F)\n",
    "kernel_resized = cv2.resize(kernel, (400, 400))  # Resize image\n",
    "\n",
    "\n",
    "img_data_array=[]\n",
    "class_name=[]\n",
    "#give path only till train folder\n",
    "# Load Dataset from give Folder \n",
    "img_folder=\"C:\\\\Users\\\\luhar\\\\emotion\\\\emotion classifier\\\\ck+\"\n",
    "for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path)\n",
    "            image=cv2.resize(image,(126,126))\n",
    "            \n",
    "            ##------------------------------Apply Gabour filter here-----------------------------##\n",
    "            image = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
    "            \n",
    "            ##------------------------------Code for Gabour filter Complete here_------------------##\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            img1 = random_noise(image,'gaussian', mean=0.1,var=0.1)\n",
    "            img_data_array.append(img1)\n",
    "            class_name.append(dir1)\n",
    "            class_name.append(dir1)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "class_name= label_encoder.fit_transform(class_name)\n",
    "\n",
    "img_data_array=np.array(img_data_array)\n",
    "img_data_array[0].shape\n",
    "# Train-Test Split\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data_array,class_name,test_size=0.1,shuffle=True)\n",
    "\n",
    "# Print shape of training and testing dataset\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of dataset without gabor filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_array=[]\n",
    "class_name=[]\n",
    "#give path only till train folder\n",
    "# Dataset LOading\n",
    "img_folder=\"C:\\\\Users\\\\luhar\\\\OneDrive\\\\Documents\\\\Code with ShiviSandy\\\\emotion_classifier\\\\ck+\"\n",
    "for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path)\n",
    "            image=cv2.resize(image,(126,126))\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "# LAbel Encoding of Dataset\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "class_name= label_encoder.fit_transform(class_name)\n",
    "img_data_array=np.array(img_data_array)\n",
    "img_data_array[0].shape\n",
    "# Train Test Split of dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data_array,class_name,test_size=0.1,shuffle=True)\n",
    "# print shape of train and test dataset\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_data_array[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the different models here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model No 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of CNN\n",
    "model=Sequential([\n",
    "        Conv2D(32,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\",input_shape=img_data_array[0].shape),\n",
    "        Conv2D(64,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\"),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Conv2D(128,kernel_size=(3,3),activation=\"relu\",padding=\"SAME\"),\n",
    "        Dropout(0.5),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(1024,activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(512,activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(7,activation=\"softmax\")\n",
    "    ])\n",
    "# COmpile the model with optimizer and loss function \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# Train the model\n",
    "history=model.fit(x_train,y_train,validation_split=0.1,epochs=30,shuffle=True)\n",
    "frame = pd.DataFrame(history.history)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Custom Model no 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building and training the model\n",
    "model=Sequential([\n",
    "    Conv2D(6,kernel_size=1,input_shape=img_data_array[0].shape),\n",
    "    ReLU(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(16,kernel_size=1),\n",
    "    ReLU(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(120,kernel_size=5),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(84,activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(7,activation='softmax')\n",
    "])\n",
    "# COmpile the model with optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# Train the model\n",
    "history=model.fit(x_train,y_train,validation_split=0.1,epochs=30)\n",
    "frame = pd.DataFrame(history.history)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f06bff",
   "metadata": {},
   "source": [
    "Plotting the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph of loss vs no of Epochs\n",
    "frame1=frame.plot(y=\"loss\",title=\"Epocs vs loss\")\n",
    "frame1.set(xlabel=\"Epocs\",ylabel=\"loss\")\n",
    "frame1.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph between accuracy and no of epochs\n",
    "frame.plot( y=[\"accuracy\",\"val_accuracy\"], figsize=(9, 8))\n",
    "plt.title(\"Epoch vs accuracy\")\n",
    "plt.xlabel(\"# of Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model\n",
    "loss,acc=model.evaluate(x_test,y_test,verbose=2)\n",
    "print(loss)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d465d72bb9e7834e44b633b17b36e1f56be332276ee2509d196ad2ec1e0a733a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
