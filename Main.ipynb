{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importing important Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CV2' :- Module to do Computer Vision Task\n",
    "import cv2\n",
    "# 'NUMPY' :- MOdule to do operation on array\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# 'PANDAS':- Modules to do operation on Dataframes\n",
    "import pandas as pd\n",
    "# 'SKlearn' :- To do operation on Datasets and taking other Algorithm for training\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'MATPLOTLIB':- Module to draw graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing functions from tensorflow for building and training the model\n",
    "# It is used to do operation on NN models. \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,InputLayer,Conv2D,MaxPooling2D,Dropout,MaxPool2D,Softmax,ReLU,GlobalAveragePooling2D\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION OF IMAGES\n",
    "def mat2gray(img):\n",
    "    A = np.double(img)\n",
    "    out = np.zeros(A.shape, np.double)\n",
    "    # MIN_MAX Normalization\n",
    "    normalized = cv2.normalize(A, out, 1.0, 0.0, cv2.NORM_MINMAX)\n",
    "    return out\n",
    "# ADDED GAUSSIAN RANDOM NOISE FOR AGUMENTATION\n",
    "def random_noise(image, mode='gaussian', seed=None, clip=True, **kwargs):\n",
    "    image = mat2gray(image)\n",
    "    mode = mode.lower()\n",
    "    if image.min() < 0:\n",
    "        low_clip = -1\n",
    "    else:\n",
    "        low_clip = 0\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "    if mode == 'gaussian':\n",
    "        noise = np.random.normal(kwargs['mean'], kwargs['var'] ** 0.5,image.shape)        \n",
    "        out = image  + noise\n",
    "    if clip:        \n",
    "        out = np.clip(out, low_clip, 1.0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of dataset with gabor filter and augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Gabor Filter\n",
    "\n",
    "ksize =18  \n",
    "sigma = 1.5  \n",
    "theta = 3*np.pi/4 \n",
    "lamda = 5  \n",
    "gamma=1.5 \n",
    "phi = 0\n",
    "\n",
    "kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, phi, ktype=cv2.CV_32F)\n",
    "kernel_resized = cv2.resize(kernel, (400, 400))  # Resize image\n",
    "\n",
    "\n",
    "img_data_array=[]\n",
    "class_name=[]\n",
    "#give path only till train folder\n",
    "# Load Dataset from give Folder \n",
    "img_folder=r\"C:\\Users\\luhar\\Projects\\Emotion_Classfier\\dataset CK+\\ck+\"\n",
    "for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path)\n",
    "            image=cv2.resize(image,(126,126))\n",
    "            \n",
    "            ##------------------------------Apply Gabour filter here-----------------------------##\n",
    "            image = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
    "            \n",
    "            ##------------------------------Code for Gabour filter Complete here_------------------##\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            img1 = random_noise(image,'gaussian', mean=0.1,var=0.1)\n",
    "            img_data_array.append(img1)\n",
    "            class_name.append(dir1)\n",
    "            class_name.append(dir1)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "class_name= label_encoder.fit_transform(class_name)\n",
    "\n",
    "img_data_array=np.array(img_data_array)\n",
    "img_data_array[0].shape\n",
    "# Train-Test Split\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data_array,class_name,test_size=0.15,shuffle=True,random_state=42)\n",
    "\n",
    "# Print shape of training and testing dataset\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of dataset without gabor filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sample of output message\n",
    "plt.imshow(img_data_array[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the different models here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model No 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of CNN\n",
    "with tf.device(\"cpu:0\"):\n",
    "    model=Sequential([\n",
    "            Conv2D(32,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\",input_shape=img_data_array[0].shape),\n",
    "            Conv2D(64,kernel_size=(5,5),activation=\"relu\",padding=\"SAME\"),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(128,kernel_size=(3,3),activation=\"relu\",padding=\"SAME\"),\n",
    "            Dropout(0.5),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Flatten(),\n",
    "            Dense(1024,activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(512,activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(7,activation=\"softmax\")\n",
    "        ])\n",
    "# COmpile the model with optimizer and loss function \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7da5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"cpu:0\"):\n",
    "    history=model.fit(x_train,y_train,validation_split=0.1,epochs=30,shuffle=True,)\n",
    "    frame = pd.DataFrame(history.history)\n",
    "    print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1197ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Custom Model no 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f06bff",
   "metadata": {},
   "source": [
    "Plotting the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph between accuracy and no of epochs\n",
    "frame.plot( y=[\"loss\",\"val_loss\"], figsize=(9, 8))\n",
    "plt.title(\"Epoch vs loss\")\n",
    "plt.xlabel(\"# of Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph between accuracy and no of epochs\n",
    "frame.plot( y=[\"accuracy\",\"val_accuracy\"], figsize=(9, 8))\n",
    "plt.title(\"Epoch vs accuracy\")\n",
    "plt.xlabel(\"# of Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "with tf.device(\"cpu:0\"):\n",
    "    loss,acc=model.evaluate(x_test,y_test,verbose=2)\n",
    "    print(loss)\n",
    "    print(acc)\n",
    "    y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arg=y_pred.argmax(axis=1)\n",
    "print(y_pred_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d597ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred_arg))\n",
    "print(classification_report(y_test,y_pred_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8218482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "array= np.array(\n",
    "    [[17 , 2 , 5 , 0 , 0 , 0 , 1],[ 1 ,19,  6 , 0 , 0 , 0 , 0],[ 0,  0, 30 , 0 , 0,  0,  0],[ 0 , 0 , 0 ,13 , 0 , 0 , 0],[ 0 , 0 , 0 , 0 ,27 , 0 , 0],[ 1, 1 , 1,  0 , 0 ,11 , 2],[ 2 ,0,  0 , 0  ,0  ,0 ,25]]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574100e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(array,annot=True,)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d465d72bb9e7834e44b633b17b36e1f56be332276ee2509d196ad2ec1e0a733a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
